{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os.path\n",
    "\n",
    "def make_tarfile(output_filename, source_dir):\n",
    "    with tarfile.open(output_filename, \"w:gz\") as tar:\n",
    "        tar.add(source_dir, arcname=os.path.basename(source_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import progressbar\n",
    "import hdfs\n",
    "from os import path\n",
    "client2 = hdfs.InsecureClient('http://localhost:9870', user='root')\n",
    "client2.makedirs(\"/random-dot\")\n",
    "client2.makedirs(\"/random-dot/train-data\")\n",
    "hdfs_path = '/random-dot/train-data'\n",
    "train_data_folder = f'C:\\\\Users\\\\rmsry\\\\Data_Analytics\\\\CA3\\\\random-dot\\\\magiceye_gen\\\\shapes\\\\stereograms'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_images = 20000\n",
    "bar = progressbar.ProgressBar(max_value=total_images)\n",
    "barprogress = 0\n",
    "\n",
    "def progress_bar(file, b):\n",
    "    global barprogress\n",
    "    global bar\n",
    "    if(b == -1):\n",
    "        barprogress += 1\n",
    "        bar.update(barprogress)\n",
    "\n",
    "def upload_shape(shape_name:str, hdfs_path='/random-dot/train-data'):\n",
    "    hdfs_path = f'/random-dot/train-data'\n",
    "    local_path = f'{train_data_folder}\\\\{shape_name}'\n",
    "    # Create shape folder\n",
    "    client2.makedirs(hdfs_path)\n",
    "    # Upload stereograms images\n",
    "    try:\n",
    "        client2.upload(hdfs_path=hdfs_path, local_path=local_path, progress=progress_bar)\n",
    "    except hdfs.util.HdfsError:\n",
    "        global barprogress\n",
    "        global bar\n",
    "        barprogress += 1\n",
    "        bar.update(barprogress)\n",
    "        pass\n",
    "    \n",
    "    \n",
    "def upload_all():\n",
    "    upload_shape(\"circle\")\n",
    "    upload_shape(\"square\")\n",
    "    upload_shape(\"star\")\n",
    "    upload_shape(\"triangle\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload all images to hadoop hdfs\n",
    "\n",
    "run upload_all() to move all images into hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload training images to hdfs\n",
    "traing-shapes.tgz 2.9Gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compressed file\n",
    "compress_train_images = f'{train_data_folder}\\\\train-shapes.tgz'\n",
    "if path.exists(compress_train_images) is False:\n",
    "    make_tarfile(compress_train_images, train_data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload file to hadoop hdfs\n",
    "client2.upload(hdfs_path=hdfs_path, local_path=compress_train_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload subset of images for development "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import progressbar\n",
    "from os import path, mkdir\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "\n",
    "train_data_folder = f'C:\\\\Users\\\\rmsry\\\\Data_Analytics\\\\CA3\\\\random-dot\\\\magiceye_gen\\\\shapes\\\\stereograms'\n",
    "batch_size = 32\n",
    "img_height = 1024\n",
    "img_width = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18435 files belonging to 4 classes.\n",
      "Using 1844 files for training.\n"
     ]
    }
   ],
   "source": [
    "#take only 10% of images for development\n",
    "dataset_images = tf.keras.utils.image_dataset_from_directory(\n",
    "  train_data_folder,\n",
    "  validation_split=0.9,\n",
    "  subset=\"training\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['circle', 'square', 'star', 'triangle']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_images.class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97% (1798 of 1844) |################### | Elapsed Time: 0:00:01 ETA:   0:00:00"
     ]
    }
   ],
   "source": [
    "#set progress bar\n",
    "bar = progressbar.ProgressBar(max_value=len(dataset_images.file_paths))\n",
    "barprogress = 0\n",
    "targetDir = f'C:\\\\Users\\\\rmsry\\\\Data_Analytics\\\\CA3\\\\random-dot\\\\magiceye_gen\\\\shapes\\\\stereograms-dev'\n",
    "\n",
    "# create folder structure\n",
    "if path.exists(targetDir) is False:\n",
    "    mkdir(targetDir)\n",
    "    for c in dataset_images.class_names:\n",
    "        mkdir(f'{targetDir}\\\\{c}')\n",
    "\n",
    "        # copy file for development\n",
    "for file in dataset_images.file_paths:\n",
    "    destFile = file.replace(train_data_folder,targetDir)\n",
    "    shutil.copy(file,destFile)\n",
    "    barprogress += 1\n",
    "    bar.update(barprogress)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload compressed file\n",
    "compress_development_images = f'{targetDir}\\\\train-shapes-dev.tgz'\n",
    "if path.exists(compress_development_images) is False:\n",
    "    make_tarfile(compress_development_images, targetDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/random-dot/train-data/train-shapes-dev.tgz'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upload file to hadoop hdfs\n",
    "client2.upload(hdfs_path=hdfs_path, local_path=compress_development_images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ca3",
   "language": "python",
   "name": "ca3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "4e1610b46226b22852eac786b2474b119f4174bb65c0340355cc8090bff37076"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
